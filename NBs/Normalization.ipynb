{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean as _mean, stddev as _stddev, col , collect_list\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy\n",
    "from numpy import allclose\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"My Spark App\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_genre_df = spark.read.csv(\"cleaned_songs.csv\", header=True, inferSchema=True)\n",
    "features_genre_df = features_genre_df.na.drop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col_name for col_name in features_genre_df.columns if col_name != \"genre\"]\n",
    "\n",
    "for feature in features:\n",
    "    features_genre_df = features_genre_df.filter( col(feature).cast(\"float\").isNotNull() | col(feature).cast(\"int\").isNotNull() | col(feature).cast(\"double\").isNotNull() )\n",
    "print(features_genre_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for feature in features:\n",
    "#     stats = cleaned_df.select(\n",
    "#         _mean(col(feature)).alias(\"mean\"), _stddev(col(feature)).alias(\"stddev\")\n",
    "#     ).collect()\n",
    "#     mean = stats[0][\"mean\"]\n",
    "#     stddev = stats[0][\"stddev\"]\n",
    "\n",
    "#     # Normalize each feature using z-scoring\n",
    "#     cleaned_df = cleaned_df.withColumn(feature, (col(feature) - mean) / stddev)\n",
    "\n",
    "#     # make all values lies between -1 and 1\n",
    "\n",
    "#     min_value = cleaned_df.agg({feature: \"min\"}).collect()[0][0]\n",
    "#     max_value = cleaned_df.agg({feature: \"max\"}).collect()[0][0]\n",
    "\n",
    "#     value = max(abs(min_value), abs(max_value))\n",
    "\n",
    "#     cleaned_df = cleaned_df.withColumn(feature, (col(feature) / value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to map each value to a tuple of (value, value^2, 1)\n",
    "def z_score_map_function(row):\n",
    "    value = row   \n",
    "    return [(value, value**2, 1)]\n",
    "\n",
    "# Function to reduce tuples of (value, value^2, 1) to (sum, sum of squares, count)\n",
    "def z_score_reduce_function(acc, value):\n",
    "    return (acc[0] + value[0], acc[1] + value[1], acc[2] + value[2])\n",
    "    \n",
    "# Function to get absolute maximum value \n",
    "def abs_max_map_function(x):\n",
    "    return [(abs(x[0]))]\n",
    "\n",
    "# Function to get absolute maximum value\n",
    "def abs_max_reduce_function(acc, value):\n",
    "    return max(acc, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "for index , feature in enumerate(features):\n",
    "    # cast to double\n",
    "    features_genre_df = features_genre_df.withColumn(feature, col(feature).cast(DoubleType()))\n",
    "    # Map step\n",
    "    mapped_rdd = features_genre_df.rdd.flatMap(lambda x: z_score_map_function(x[index+1]))\n",
    "    \n",
    "    # Reduce step\n",
    "    reduced_result = mapped_rdd.reduce(lambda acc, value: z_score_reduce_function(acc, value))\n",
    "\n",
    "    # Calculate standard deviation\n",
    "    sum_value = reduced_result[0]\n",
    "    sum_of_squares = reduced_result[1]\n",
    "    count = reduced_result[2]\n",
    "\n",
    "    mean = sum_value / count\n",
    "    variance = (sum_of_squares - (sum_value**2 / count)) / count\n",
    "    stddev = variance ** 0.5\n",
    "    \n",
    "    #perform z-score normalization\n",
    "    features_genre_df = features_genre_df.withColumn(feature, (col(feature) - mean) / stddev)\n",
    "    \n",
    "    # make all values lies between -1 and 1\n",
    "    \n",
    "    # get absolute maximum value\n",
    "    mapped_rdd = features_genre_df.select(feature).rdd.flatMap(lambda x: abs_max_map_function(x))\n",
    "    reduced_result = mapped_rdd.reduce(lambda acc, value: abs_max_reduce_function(acc, value))\n",
    "    \n",
    "    max_value = reduced_result\n",
    "    \n",
    "    features_genre_df = features_genre_df.withColumn(feature, (col(feature) / max_value))\n",
    "\n",
    "    print(\"Standard Deviation:\", stddev)\n",
    "    print(\"Mean:\", mean)\n",
    "    print (\"Max Value:\", max_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "features_genre_df.toPandas().to_csv(\"normalized_songs.csv\", index=False , header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the string column 'genre' to a numerical column 'indexed_genre'\n",
    "stringIndexer = StringIndexer(inputCol=\"genre\", outputCol=\"indexed_genre\")\n",
    "si_model = stringIndexer.fit(features_genre_df)\n",
    "td = si_model.transform(features_genre_df)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\" , handleInvalid=\"skip\")\n",
    "td = assembler.transform(td)\n",
    "\n",
    "\n",
    "# split the data into train and test sets\n",
    "td = td.select(\"indexed_genre\" , \"features\")\n",
    "\n",
    "td.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(td.limit(10).toPandas())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "train_data, test_data = td.randomSplit([0.8, 0.2], seed=42)\n",
    "train_labels= train_data.select(\"indexed_genre\").collect()\n",
    "# Initialize the RandomForestClassifier\n",
    "rf = RandomForestClassifier(numTrees=30, maxDepth=20, labelCol=\"indexed_genre\", seed=42,\n",
    "                            leafCol=\"leafId\")\n",
    "\n",
    "# Train the model on the training data\n",
    "model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "genres = set(train_labels)\n",
    "genres = list(genres)\n",
    "def get_score_confusion_matrix(model, test_Data, test_labels):\n",
    "    \n",
    "    score_matrix = [[0 for i in range(len(genres))] for j in range(len(genres))]\n",
    "    \n",
    "    for i in range(len(test_labels)):\n",
    "        pred = model.predict([test_Data[i]])\n",
    "        score_matrix[genres.index(test_labels[i])][genres.index(pred[0])] += 1\n",
    "    #normalize the confusion matrix\n",
    "    score_matrix = [[score_matrix[i][j]/sum(score_matrix[i]) for j in range(len(genres))] for i in range(len(genres))]\n",
    "    return score_matrix\n",
    "\n",
    "def draw_confusion_matrix(confusion_matrix):\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    sns.heatmap(confusion_matrix, annot=True, cmap='Blues', xticklabels=genres, yticklabels=genres , fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best score: 0.5236602052451539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best score: 0.5653010723123454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best score: 0.47594171020071485"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_tata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
